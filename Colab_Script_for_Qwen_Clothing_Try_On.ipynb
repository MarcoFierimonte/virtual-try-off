{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoFierimonte/virtual-try-off/blob/main/Colab_Script_for_Qwen_Clothing_Try_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the **Qwen Image Edit Clothing Try-On** application on a free Google Colab GPU (T4, 16GB VRAM), we must use memory optimizations because the base model is quite large (~20B parameters).\n",
        "\n",
        "The following script installs the latest `diffusers` library, loads the **Qwen-Image-Edit-2509** model with 4-bit quantization (to fit into the T4's VRAM), and implements the two-step pipeline (Extract $\\rightarrow$ Try-On) described in the DigitalOcean tutorial.\n",
        "\n",
        "### Google Colab Script"
      ],
      "metadata": {
        "id": "0UovU6RmjY_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary dependencies\n",
        "!pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -U transformers accelerate peft bitsandbytes sentencepiece gradio\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import QwenImageEditPlusPipeline\n",
        "from diffusers.utils import load_image\n",
        "import gradio as gr\n",
        "\n",
        "# 2. Setup the Pipeline with Memory Optimizations\n",
        "# We use 4-bit quantization and CPU offloading to fit a 20B model on a T4 GPU\n",
        "model_id = \"Qwen/Qwen-Image-Edit-2509\"\n",
        "\n",
        "print(\"Loading model... This may take a few minutes.\")\n",
        "pipe = QwenImageEditPlusPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\", # Automatically handles memory across CPU/GPU\n",
        "    load_in_4bit=True  # Crucial for Colab Free T4\n",
        ")\n",
        "\n",
        "# 3. Define the Extraction Function\n",
        "def extract_clothes(input_image):\n",
        "    # Load the specific LoRA for clothing extraction\n",
        "    pipe.load_lora_weights(\n",
        "        \"JamesDigitalOcean/Qwen_Image_Edit_Extract_Clothing\",\n",
        "        weight_names=\"qwen_image_edit_remove_body.safetensors\",\n",
        "        adapter_name=\"removebody\"\n",
        "    )\n",
        "\n",
        "    # Process image\n",
        "    if isinstance(input_image, str):\n",
        "        pil_image = load_image(input_image)\n",
        "    else:\n",
        "        pil_image = Image.fromarray(input_image).convert(\"RGB\")\n",
        "\n",
        "    prompt = \"removebody remove the person from this image, but leave the outfit. the clothes should remain after deleting the person's body, skin, and hair. leave the clothes in front of a white background\"\n",
        "\n",
        "    image = pipe(\n",
        "        image=[pil_image],\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30 # Reduced for speed\n",
        "    ).images[0]\n",
        "\n",
        "    pipe.unload_lora_weights() # Clear weights to save VRAM\n",
        "    return image\n",
        "\n",
        "# 4. Define the Try-On Function\n",
        "def tryon_clothes(clothing_image, person_image):\n",
        "    # Load the specific LoRA for virtual try-on\n",
        "    pipe.load_lora_weights(\n",
        "        \"JamesDigitalOcean/Qwen_Image_Edit_Try_On_Clothes\",\n",
        "        weight_names=\"qwen_image_edit_tryon.safetensors\",\n",
        "        adapter_name=\"tryonclothes\"\n",
        "    )\n",
        "\n",
        "    # Ensure images are PIL\n",
        "    pil_clothes = Image.fromarray(clothing_image).convert(\"RGB\") if not isinstance(clothing_image, Image.Image) else clothing_image\n",
        "    pil_person = Image.fromarray(person_image).convert(\"RGB\") if not isinstance(person_image, Image.Image) else person_image\n",
        "\n",
        "    # Prompt logic from tutorial: Person on top, clothing on bottom\n",
        "    prompt = \"tryonclothes put the set of clothing onto the person in the image. Style the person in the image using the clothing provided.\"\n",
        "\n",
        "    image = pipe(\n",
        "        image=[pil_person, pil_clothes],\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30\n",
        "    ).images[0]\n",
        "\n",
        "    pipe.unload_lora_weights()\n",
        "    return image\n",
        "\n",
        "# 5. Create Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Qwen Image Edit: Clothing Try-On\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_model = gr.Image(label=\"1. Original Photo (Model wearing clothes)\")\n",
        "            extract_btn = gr.Button(\"Step 1: Extract Clothing\")\n",
        "        with gr.Column():\n",
        "            extracted_output = gr.Image(label=\"Extracted Clothing (White Background)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            target_person = gr.Image(label=\"2. Target Person (Photo to receive clothes)\")\n",
        "            tryon_btn = gr.Button(\"Step 2: Apply Try-On\")\n",
        "        with gr.Column():\n",
        "            final_output = gr.Image(label=\"Final Result\")\n",
        "\n",
        "    # Wire up the buttons\n",
        "    extract_btn.click(fn=extract_clothes, inputs=input_model, outputs=extracted_output)\n",
        "    tryon_btn.click(fn=tryon_clothes, inputs=[extracted_output, target_person], outputs=final_output)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wkK7jCcMjY_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Setup Instructions for Colab:\n",
        "\n",
        "1. **Change Runtime**: Go to `Runtime` -> `Change runtime type` and select **T4 GPU**.\n",
        "2. **Memory Warning**: Even with 4-bit quantization, this model is extremely large for a T4. If you encounter an \"Out of Memory\" (OOM) error, try restarting the session and running the code again without running other cells.\n",
        "3. **Authentication**: If the model asks for a Hugging Face token, you may need to run `!huggingface-cli login` in a cell beforehand, as some Qwen weights require accepting a license agreement on their Hugging Face page.\n",
        "\n",
        "[Qwen Image Edit Try-On Tutorial](https://www.youtube.com/watch?v=CY9U59mcWqQ)\n",
        "\n",
        "This video provides a visual walkthrough of the Qwen Image Edit LoRA workflow for virtual try-ons, which is helpful for understanding how to align the input images and prompts for the best results."
      ],
      "metadata": {
        "id": "ltJej0jjjY_f"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}