{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoFierimonte/virtual-try-off/blob/main/Colab_Script_Fixes_for_Qwen_Clothing_Try_On_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This updated script implements the **Google Cloud Storage (GCS) Mounting** fix. This allows the 20GB+ model weights to be saved directly to your cloud bucket, bypassing the small local disk limit of the free Colab tier.\n",
        "\n",
        "### Prerequisites:\n",
        "\n",
        "1. **Create a GCS Bucket:** Go to your Google Cloud Console and create a bucket (e.g., `my-qwen-storage`).\n",
        "2. **Hugging Face Token:** You will need a Read token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
        "\n",
        "### New Colab Script:"
      ],
      "metadata": {
        "id": "FL4gVlyuK3r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 1. AUTHENTICATION & GCS MOUNTING\n",
        "# ==========================================================\n",
        "from google.colab import auth\n",
        "import os\n",
        "\n",
        "# LOGIN to Google to access your Cloud Storage\n",
        "auth.authenticate_user()\n",
        "\n",
        "# SET YOUR BUCKET NAME HERE\n",
        "BUCKET_NAME = \"your-unique-bucket-name\" # <--- CHANGE THIS\n",
        "MOUNT_PATH = \"/content/qwen_data\"\n",
        "\n",
        "# Install gcsfuse to mount the bucket\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-focal main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt-get update\n",
        "!apt-get install gcsfuse\n",
        "\n",
        "# Create directory and mount the bucket\n",
        "!mkdir -p {MOUNT_PATH}\n",
        "!gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_PATH}\n",
        "\n",
        "# REDIRECT HUGGINGFACE CACHE TO THE BUCKET\n",
        "# This ensures the 20GB model goes to the cloud, not the local disk\n",
        "os.environ['HF_HOME'] = os.path.join(MOUNT_PATH, \"hf_cache\")\n",
        "\n",
        "# ==========================================================\n",
        "# 2. INSTALL DEPENDENCIES\n",
        "# ==========================================================\n",
        "!pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -U transformers accelerate peft bitsandbytes sentencepiece gradio\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import QwenImageEditPlusPipeline\n",
        "from diffusers.utils import load_image\n",
        "import gradio as gr\n",
        "\n",
        "# ==========================================================\n",
        "# 3. SETUP MODEL (WITH 4-BIT OPTIMIZATION)\n",
        "# ==========================================================\n",
        "model_id = \"Qwen/Qwen-Image-Edit-2509\"\n",
        "\n",
        "print(\"Loading model into GCS Bucket... This may take 10+ mins on the first run.\")\n",
        "# load_in_4bit=True is required to fit the 16GB VRAM of the T4 GPU\n",
        "pipe = QwenImageEditPlusPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# 4. DEFINE FUNCTIONS\n",
        "# ==========================================================\n",
        "\n",
        "def extract_clothes(input_image):\n",
        "    # Load LoRA for extraction\n",
        "    pipe.load_lora_weights(\n",
        "        \"JamesDigitalOcean/Qwen_Image_Edit_Extract_Clothing\",\n",
        "        weight_names=\"qwen_image_edit_remove_body.safetensors\",\n",
        "        adapter_name=\"removebody\"\n",
        "    )\n",
        "\n",
        "    if isinstance(input_image, str):\n",
        "        pil_image = load_image(input_image)\n",
        "    else:\n",
        "        pil_image = Image.fromarray(input_image).convert(\"RGB\")\n",
        "\n",
        "    prompt = \"removebody remove the person from this image, but leave the outfit. the clothes should remain after deleting the person's body, skin, and hair. leave the clothes in front of a white background\"\n",
        "\n",
        "    image = pipe(\n",
        "        image=[pil_image],\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30\n",
        "    ).images[0]\n",
        "\n",
        "    pipe.unload_lora_weights()\n",
        "    return image\n",
        "\n",
        "def tryon_clothes(clothing_image, person_image):\n",
        "    # Load LoRA for try-on\n",
        "    pipe.load_lora_weights(\n",
        "        \"JamesDigitalOcean/Qwen_Image_Edit_Try_On_Clothes\",\n",
        "        weight_names=\"qwen_image_edit_tryon.safetensors\",\n",
        "        adapter_name=\"tryonclothes\"\n",
        "    )\n",
        "\n",
        "    pil_clothes = Image.fromarray(clothing_image).convert(\"RGB\") if not isinstance(clothing_image, Image.Image) else clothing_image\n",
        "    pil_person = Image.fromarray(person_image).convert(\"RGB\") if not isinstance(person_image, Image.Image) else person_image\n",
        "\n",
        "    prompt = \"tryonclothes put the set of clothing onto the person in the image. Style the person in the image using the clothing provided.\"\n",
        "\n",
        "    image = pipe(\n",
        "        image=[pil_person, pil_clothes],\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30\n",
        "    ).images[0]\n",
        "\n",
        "    pipe.unload_lora_weights()\n",
        "    return image\n",
        "\n",
        "# ==========================================================\n",
        "# 5. GRADIO UI\n",
        "# ==========================================================\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Qwen Image Edit: Clothing Try-On (GCS Powered)\")\n",
        "    gr.Markdown(\"Using Cloud Storage to bypass Colab Disk Limits.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_model = gr.Image(label=\"1. Original Photo (Model wearing clothes)\")\n",
        "            extract_btn = gr.Button(\"Step 1: Extract Clothing\")\n",
        "        with gr.Column():\n",
        "            extracted_output = gr.Image(label=\"Extracted Clothing (White Background)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            target_person = gr.Image(label=\"2. Target Person (Photo to receive clothes)\")\n",
        "            tryon_btn = gr.Button(\"Step 2: Apply Try-On\")\n",
        "        with gr.Column():\n",
        "            final_output = gr.Image(label=\"Final Result\")\n",
        "\n",
        "    extract_btn.click(fn=extract_clothes, inputs=input_model, outputs=extracted_output)\n",
        "    tryon_btn.click(fn=tryon_clothes, inputs=[extracted_output, target_person], outputs=final_output)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dhEeapihK3sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why this works:\n",
        "\n",
        "1. **`gcsfuse`**: This \"tricks\" the computer into thinking your Google Cloud bucket is just a folder.\n",
        "2. **`os.environ['HF_HOME']`**: By pointing this variable to your bucket folder, Hugging Face will download all those massive model files (20GB+) directly into your cloud storage. Your Colab \"Disk Space\" bar will stay green.\n",
        "3. **`load_in_4bit`**: This compresses the model so it fits into the 16GB VRAM of the free T4 GPU. Without this, you would get an \"Out of Memory\" (OOM) error immediately."
      ],
      "metadata": {
        "id": "6x2hRXDpK3sB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}